<h2>Project Goal:</h2> 

The goal of this project is to implement a Bigram Language Model with Transfer Learning, incorporating concepts from the GPT-2 model. Leveraging pre-trained weights from GPT-2, the objective is to enhance the language model's ability to predict the next token in a sequence, demonstrating the effectiveness of transfer learning in natural language processing tasks.

<h2>Project Description:</h2> 

This project focuses on building a Bigram Language Model that benefits from the knowledge captured by a pre-trained GPT-2 model. The model is designed to predict the next token in a sequence, offering a versatile foundation for various natural language generation tasks. By incorporating transfer learning, the model inherits the linguistic nuances and patterns learned during pre-training, enabling it to excel in capturing context-specific relationships in the provided dataset. The implementation showcases the seamless integration of transfer learning techniques to boost the performance of language models, making it a valuable resource for researchers and practitioners interested in advanced language modeling.


[Click here to view my Tech-Doc presentation](https://docs.google.com/presentation/d/e/2PACX-1vRlmiTxqwlZpKDQatTE3NdY2ltgxc8OrSqAVcOh5UhiOhk9q8WRdZ6KoVffb0O6vBcw551psVe4fjd-/pub?start=true&loop=false&delayms=3000)
